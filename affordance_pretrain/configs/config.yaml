# Unified Affordance Pre-training Configuration
# Optimized for better convergence

# Model configuration
model:
  d_tr: 256
  num_human_points: 10475

  # VLM settings
  use_lightweight_vlm: false
  vlm_model_name: "llava-hf/llava-1.5-7b-hf"

  # LoRA settings
  lora_r: 32              # Increased from 16 for more capacity
  lora_alpha: 64          # Increased proportionally (alpha = 2 * r)
  lora_dropout: 0.1       # Slightly higher dropout
  freeze_vlm: false

  dropout: 0.2            # Reduced from 0.3

# Data configuration
data:
  damon_root: "/inspire/qb-ilm/project/robot-reasoning/xiangyushun-p-xiangyushun/boran/DATASET/DAMON"
  piad_root: "/inspire/qb-ilm/project/robot-reasoning/xiangyushun-p-xiangyushun/boran/DATASET/PIAD"
  piad_setting: "Seen"

  image_size: 224
  num_object_points: 2048
  num_human_points: 10475

# Training configuration - OPTIMIZED
training:
  batch_size: 4
  num_epochs: 100         # Increased epochs
  learning_rate: 3.0e-4   # Increased from 1e-4 (3x higher)
  weight_decay: 1.0e-5    # Reduced from 1e-4 (less regularization)
  warmup_epochs: 3        # Reduced from 5 (faster warmup)

  # Learning rate scheduler with warm restarts (helps escape plateaus)
  use_warm_restarts: true
  restart_period: 10      # Restart LR every 10 epochs initially (then 20, 40, ...)

  # Loss weights - adjusted for class imbalance
  lambda_human: 1.0
  lambda_object: 1.0
  use_focal: true
  use_dice: true
  focal_alpha: 0.75       # Increased from 0.25 (more weight on positive class)
  focal_gamma: 2.0
  lambda_dice: 2.0        # Increased Dice weight (better for imbalanced data)
  lambda_focal: 1.0

  # Learning rate groups - less aggressive scaling
  lr_vlm_scale: 0.2       # Increased from 0.1
  lr_encoder_scale: 1.0   # Increased from 0.5
  lr_decoder_scale: 1.0

# Logging
logging:
  log_interval: 20
  save_interval: 5
  use_wandb: true
  wandb_project: "affordance-pretrain-lora"
  wandb_run_name: null

# Paths
paths:
  save_dir: "./checkpoints"
  log_dir: "./logs"
